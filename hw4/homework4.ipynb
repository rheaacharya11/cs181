{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# **CS 181 HW4 Problem 2**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Only code you have to write are places marked with a TODO comment."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Loading datasets for K-Means and HAC:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from seaborn import heatmap\n",
    "from scipy.spatial import distance\n",
    "\n",
    "small_dataset = np.load(\"data/small_dataset.npy\")\n",
    "small_labels = np.load(\"data/small_dataset_labels.npy\").astype(int)\n",
    "large_dataset = np.load(\"data/large_dataset.npy\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## K-Means"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "class KMeans(object):\n",
    "    # K is the K in KMeans\n",
    "    def __init__(self, K):\n",
    "        self.K = K\n",
    "\n",
    "    def distance(pt1, pt2):\n",
    "        return np.linalg.norm(pt1 - pt2)\n",
    "\n",
    "    def update_assignments(self):\n",
    "        for i, pt in enumerate(self.X):\n",
    "            dist_from_pt = np.array(self.distance(pt, self.centers[k]) for k in range(self.K))\n",
    "            self.assignments[i] = int(np.argmin(dist_from_pt))\n",
    "\n",
    "    def update_centers(lst):\n",
    "        for k in range(self.K):\n",
    "            # find all the ones that equal the cluster\n",
    "            curr_class = self.X[np.array(self.assignments) == k]\n",
    "            if sum(np.array(self.assignments) == k) == 0: continue\n",
    "            # take the mean\n",
    "            self.centers[k] = np.mean(class_values, axis = 0)\n",
    "    \n",
    "    def update_objective(self):\n",
    "        # this basically adds the objective for this iteration\n",
    "        cumulative = 0\n",
    "        for i, c in enumerate(self.assignments): # i says index overall, c is the cluster assigned to\n",
    "            # add distance between it and its point\n",
    "            cumulative += self.distance(self.X[i], self.centers[c])**2\n",
    "        # adds to overall list of objectives\n",
    "        self.objectives.append(cumulative)\n",
    "\n",
    "    # X is a (N x 28 x 28) array where 28x28 is the dimensions of each of the N images.\n",
    "    def fit(self, X):\n",
    "        # TODO: implement this function\n",
    "        self.centers = np.random.randn(self.K, self.X.shape[1]) # initialize k means; shape = [k x 784]\n",
    "        self.X = X\n",
    "        self.assignments = np.zeros(self.N)\n",
    "        # we do 10 iterations\n",
    "        for i in range(10):\n",
    "            # we assign each data point to the median that it is closest to\n",
    "            print(\"Iteration\", i)\n",
    "            self.update_assignments\n",
    "            self.update_objective\n",
    "            self.update_centers\n",
    "            for pt in X:\n",
    "                ds = []\n",
    "                for cluster_mean in self.means:\n",
    "                    ds.append(distance(pt, cluster_mean))\n",
    "                self.assignment.append(np.argmin(ds))\n",
    "\n",
    "            # we update new means:\n",
    "            self.means = update_means(self.assignment)\n",
    "\n",
    "        return means           \n",
    "        pass\n",
    "\n",
    "    # This should return the arrays for K images. Each image should represent the mean of each of the fitted clusters.\n",
    "    def get_mean_images(self):\n",
    "        # TODO: change this!\n",
    "        return small_dataset[:self.K]\n",
    "    \n",
    "    # TODO: change this for part 1's plot\n",
    "    def plot_verify_objective(self):\n",
    "        \n",
    "        pass\n",
    "    \n",
    "    # TODO: change this for part 5\n",
    "    def get_cluster_sizes(self):\n",
    "        pass"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Part 1: This takes seconds to run"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "'KMeans' object has no attribute 'X'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[5], line 2\u001b[0m\n\u001b[1;32m      1\u001b[0m KMeansClassifier \u001b[38;5;241m=\u001b[39m KMeans(K\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m10\u001b[39m)\n\u001b[0;32m----> 2\u001b[0m KMeansClassifier\u001b[38;5;241m.\u001b[39mfit(large_dataset)\n\u001b[1;32m      3\u001b[0m KMeansClassifier\u001b[38;5;241m.\u001b[39mplot_verify_objective()\n",
      "Cell \u001b[0;32mIn[4], line 34\u001b[0m, in \u001b[0;36mKMeans.fit\u001b[0;34m(self, X)\u001b[0m\n\u001b[1;32m     32\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mfit\u001b[39m(\u001b[38;5;28mself\u001b[39m, X):\n\u001b[1;32m     33\u001b[0m     \u001b[38;5;66;03m# TODO: implement this function\u001b[39;00m\n\u001b[0;32m---> 34\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcenters \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39mrandom\u001b[38;5;241m.\u001b[39mrandn(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mK, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mX\u001b[38;5;241m.\u001b[39mshape[\u001b[38;5;241m1\u001b[39m]) \u001b[38;5;66;03m# initialize k means; shape = [k x 784]\u001b[39;00m\n\u001b[1;32m     35\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mX \u001b[38;5;241m=\u001b[39m X\n\u001b[1;32m     36\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39massignments \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39mzeros(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mN)\n",
      "\u001b[0;31mAttributeError\u001b[0m: 'KMeans' object has no attribute 'X'"
     ]
    }
   ],
   "source": [
    "KMeansClassifier = KMeans(K=10)\n",
    "KMeansClassifier.fit(large_dataset)\n",
    "KMeansClassifier.plot_verify_objective()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Plotting code for parts 2 and 3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_mean_image_plot(data, standardized = False):\n",
    "    niters = 3\n",
    "    K = 10\n",
    "    allmeans = np.zeros((K, niters, 784))\n",
    "    for i in range(niters):\n",
    "        KMeansClassifier = KMeans(K=K)\n",
    "        KMeansClassifier.fit(data)\n",
    "        allmeans[:,i] = KMeansClassifier.get_mean_images()\n",
    "    fig = plt.figure(figsize=(10,10))\n",
    "    plt.suptitle('Class mean images across random restarts' + (' (standardized data)' if standardized else ''), fontsize=16)\n",
    "    for k in range(K):\n",
    "        for i in range(niters):\n",
    "            ax = fig.add_subplot(K, niters, 1+niters*k+i)\n",
    "            plt.setp(ax.get_xticklabels(), visible=False)\n",
    "            plt.setp(ax.get_yticklabels(), visible=False)\n",
    "            ax.tick_params(axis='both', which='both', length=0)\n",
    "            if k == 0: plt.title('Iter '+str(i))\n",
    "            if i == 0: ax.set_ylabel('Class '+str(k), rotation=90)\n",
    "            plt.imshow(allmeans[k,i].reshape(28,28), cmap='Greys_r')\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Part 2: This takes ~3 minutes to run"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "make_mean_image_plot(large_dataset)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Part 3: This takes ~3 minutes to run"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "large_dataset_std = None # TODO\n",
    "make_mean_image_plot(large_dataset_std, standardized = True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## HAC: Part 4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class HAC(object):\n",
    "    def __init__(self, linkage):\n",
    "        self.linkage = linkage\n",
    "    \n",
    "    def fit(self, X):\n",
    "        # TODO: implement the function\n",
    "        pass\n",
    "\n",
    "    # Returns the mean image when using n_clusters clusters\n",
    "    def get_mean_images(self, n_clusters):\n",
    "        # TODO: Change this!\n",
    "        return small_dataset[:n_clusters]\n",
    "    \n",
    "    # Return assignments when there were K clusters\n",
    "    def get_k_clusters(self, K):\n",
    "        # TODO: implement the function\n",
    "        pass\n",
    "\n",
    "    # Get cluster sizes to compare min and max linkage\n",
    "    def get_cluster_sizes(self, K):\n",
    "        # TODO: implement the function\n",
    "        pass"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Part 4: This takes ~6 minutes to run"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "LINKAGES = [ 'max', 'min', 'centroid' ]\n",
    "n_clusters = 10\n",
    "cluster_sizes = []\n",
    "\n",
    "fig = plt.figure(figsize=(10,10))\n",
    "plt.suptitle(\"HAC mean images with max, min, and centroid linkagess\")\n",
    "for l_idx, l in enumerate(LINKAGES):\n",
    "    # Fit HAC\n",
    "    hac = HAC(l)\n",
    "    hac.fit(small_dataset)\n",
    "    mean_images = hac.get_mean_images(n_clusters)\n",
    "    cluster_sizes.append(hac.get_cluster_sizes(n_clusters)) # used in Part 8, append here to avoid re-fitting\n",
    "    # Make plot\n",
    "    for m_idx in range(mean_images.shape[0]):\n",
    "        m = mean_images[m_idx]\n",
    "        ax = fig.add_subplot(n_clusters, len(LINKAGES), l_idx + m_idx*len(LINKAGES) + 1)\n",
    "        plt.setp(ax.get_xticklabels(), visible=False)\n",
    "        plt.setp(ax.get_yticklabels(), visible=False)\n",
    "        ax.tick_params(axis='both', which='both', length=0)\n",
    "        if m_idx == 0: plt.title(l)\n",
    "        if l_idx == 0: ax.set_ylabel('Class '+str(m_idx), rotation=90)\n",
    "        plt.imshow(m.reshape(28,28), cmap='Greys_r')\n",
    "    print(\"Done:\", l)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Part 5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Part 5: Create the graph for comparing cluster sizes in K-means\n",
    "def plot_cluster_sizes(n_clusters, cluster_sizes):\n",
    "    fig, (ax) = plt.subplots(1, 1)\n",
    "    fig.suptitle(\"K-means Cluster Sizes\")\n",
    "    ax.set_ylabel('number of images in cluster')\n",
    "    cluster_idxs = [i for i in range(n_clusters)]\n",
    "\n",
    "    def plot_sizes_per_linkage(ax, sizes, k):\n",
    "        ax.bar(cluster_idxs, sizes)\n",
    "        ax.set_title(f'K = {k}')\n",
    "        ax.set_xlabel('cluster index')\n",
    "        ax.set_ylim(0, 1000)\n",
    "\n",
    "    plot_sizes_per_linkage(ax, cluster_sizes, str(n_clusters))\n",
    "    plt.show()\n",
    "\n",
    "plot_cluster_sizes(KMeansClassifier.K, KMeansClassifier.get_cluster_sizes())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Part 5: Create the graph for comparing cluster sizes in HAC\n",
    "def plot_cluster_sizes(n_clusters, cluster_sizes):\n",
    "    fig, (ax1, ax2, ax3) = plt.subplots(1, 3)\n",
    "    fig.suptitle(\"HAC Cluster Sizes\")\n",
    "    ax1.set_ylabel('number of images in cluster')\n",
    "    cluster_idxs = [i for i in range(n_clusters)]\n",
    "\n",
    "    def plot_sizes_per_linkage(ax, sizes, linkage):\n",
    "        ax.bar(cluster_idxs, sizes)\n",
    "        ax.set_title(f'{linkage} Linkage')\n",
    "        ax.set_xlabel('cluster index')\n",
    "        ax.set_ylim(0, 300)\n",
    "\n",
    "    plot_sizes_per_linkage(ax1, cluster_sizes[0], 'Max')\n",
    "    plot_sizes_per_linkage(ax2, cluster_sizes[1], 'Min')\n",
    "    plot_sizes_per_linkage(ax3, cluster_sizes[2], 'Centroid')\n",
    "    plt.show()\n",
    "plot_cluster_sizes(n_clusters, cluster_sizes)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Part 6: Confusion Matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: implement code to produce confusion matrics"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "6c365ea26a318e0b540d1978e97e3d03cfe51dff8cd04dae5f3d7b47d79d2453"
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
